#!/bin/bash
#PBS -l select=1:ncpus=10:mpiprocs=10:mem=100gb
#PBS -l walltime=16:00:00
#PBS -N l2_ms_hq
#PBS -P 11003280
#PBS -j oe
#PBS -o /home/project/11003280/yuli/llm-foundry/log/mdsconversion39c_ms_hq.out

# load modules
module load miniforge3/23.10

# set envvars
export HF_HOME="/home/project/11003280/vault/cache_yuli"
source /home/users/nus/huangyl/scratch/code/nscc_working/arf/mosaicml_workspace/credential/hf_token
source activate /home/project/11003280/envs/llmfoundry_yuli

# Change directory to the directory containing the data preparation script
cd /home/project/11003280/yuli/llm-foundry/scripts/data_prep
sleep 30

##########################################
# llama2
tokenizer="/home/users/nus/huangyl/scratch/model/llama2-7b-tokenizer"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/en_pile2" \
#     --out_root "/home/project/11003280/data/3lang/out_llama2/en_pile/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion33c_en_pile.out" \
#     --num_processes 64 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --eos_text "</s>" \
#     --compression "zstd" \
#     --chunk_size 300000 \
#     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/en_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/3lang/out_llama2/en_hq/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion35c_en_hq.out" \
#     --num_processes 10 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 600000 \
#     --eos_text "</s>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/id_pile1/sealion_pile.jsonl" \
#     --out_root "/home/project/11003280/data/3lang/out_llama2/id_pile1/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion36c_id_pile1.out" \
#     --num_processes 10 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 600000 \
#     --eos_text "</s>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"

#python convert_dataset_json_mp.py \
#    --path "/home/project/11003280/data/3lang/raw/id_pile2/combined.jsonl" \
#    --out_root "/home/project/11003280/data/3lang/out_llama2/id_pile2/" \
#    --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion37c_id_pile2.out" \
#    --num_processes 20 \
#    --concat_tokens 4096 \
#    --tokenizer $tokenizer \
#    --chunk_size 200000 \
#    --eos_text "</s>" \
#    --compression "zstd" \
#    |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/ms_pile/sealion_pile.jsonl" \
#     --out_root "/home/project/11003280/data/3lang/out_llama2/ms_pile/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion38c_ms_pile.out" \
#     --num_processes 10 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 600000 \
#     --eos_text "</s>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"

python convert_dataset_json_mp.py \
     --path "/home/project/11003280/data/3lang/raw/ms_hq" \
     --out_root "/home/project/11003280/data/3lang/out_llama2/ms_hq/" \
     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion39c_ms_hq.out" \
     --num_processes 10 \
     --concat_tokens 4096 \
     --tokenizer $tokenizer \
     --chunk_size 600000 \
     --eos_text "</s>" \
     --compression "zstd" \
     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/id_hq" \
#     --out_root "/home/project/11003280/data/3lang/out_llama2/id_hq/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion34c_id_hq.out" \
#     --num_processes 10 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 600000 \
#     --eos_text "</s>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"


##########################################
# llama3

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/en_pile2" \
#     --out_root "/home/project/11003280/data/3lang/out_llama3/en_pile/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion14a_en_pile.out" \
#     --num_processes 100 \
#     --concat_tokens 8192 \
#     --tokenizer "/home/users/nus/huangyl/scratch/model/llama3-8b-tokenizer" \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
#     --chunk_size 300000 \
#     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/id_pile2/combined.jsonl" \
#     --out_root "/home/project/11003280/data/3lang/out_llama3/id_pile2/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion27.out" \
#     --num_processes 15 \
#     --concat_tokens 8192 \
#     --tokenizer "/home/users/nus/huangyl/scratch/model/llama3-8b-tokenizer" \
#     --chunk_size 600000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/ms_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/3lang/out_llama3/ms_hq/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion29.out" \
#     --num_processes 10 \
#     --concat_tokens 8192 \
#     --tokenizer "/home/users/nus/huangyl/scratch/model/llama3-8b-tokenizer" \
#     --chunk_size 600000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/3lang/raw/id_hq" \
#     --out_root "/home/project/11003280/data/3lang/out_llama3/id_hq/" \
#     --log_file_path "/home/project/11003280/yuli/llm-foundry/log/mdsconversion24.out" \
#     --num_processes 10 \
#     --concat_tokens 8192 \
#     --tokenizer "/home/users/nus/huangyl/scratch/model/llama3-8b-tokenizer" \
#     --chunk_size 600000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
#     |& tee -a "$log_file"
