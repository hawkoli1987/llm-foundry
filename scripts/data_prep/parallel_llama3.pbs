#!/bin/bash
#PBS -l select=1:ncpus=35:mpiprocs=35:mem=100gb
#PBS -l walltime=16:00:00
#PBS -N l3_th_pile2
#PBS -P 11003280
#PBS -j oe
#PBS -o /home/project/11003280/log/mds_conversion/l3_th_pile2.out

# load modules
module load miniforge3/23.10

# set envvars
export HF_HOME="/home/project/11003280/vault/cache_yuli"
source /home/users/nus/huangyl/scratch/code/nscc_working/arf/mosaicml_workspace/credential/hf_token
source activate /home/project/11003280/envs/llmfoundry_yuli

# Change directory to the directory containing the data preparation script
cd /home/project/11003280/yuli/llm-foundry/scripts/data_prep
sleep 30

##########################################
# llama3
tokenizer="/home/users/nus/huangyl/scratch/model/llama3-8b-tokenizer"

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/en_pile" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/en_pile/" \
#     --num_processes 100 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
#     --chunk_size 300000 \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/en_hq/enwiki_dedup.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/en_hq/" \
#     --num_processes 20 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/th_pile1/train.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/th_pile1/" \
#     --num_processes 55 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \

python convert_dataset_json_mp.py \
    --path "/home/project/11003280/data/raw_hero/th_pile2/charin.jsonl" \
    --out_root "/home/project/11003280/data/out_hero_llama3/th_pile2/" \
    --num_processes 35 \
    --concat_tokens 4096 \
    --tokenizer $tokenizer \
    --chunk_size 300000 \
    --eos_text "<|end_of_text|>" \
    --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/th_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/th_hq/" \
#     --num_processes 2 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/vi_pile/merged.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/vi_pile/" \
#     --num_processes 100 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/vi_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_llama3/vi_hq/" \
#     --num_processes 2 \
#     --concat_tokens 4096 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<|end_of_text|>" \
#     --compression "zstd" \
