#!/bin/bash
#PBS -l select=1:ncpus=100:mpiprocs=100:mem=300gb
#PBS -l walltime=16:00:00
#PBS -N g2_en_pile
#PBS -P 11003280
#PBS -j oe
#PBS -o /home/project/11003280/log/mds_conversion/g2_en_pile.out

# load modules
module load miniforge3/23.10

# set envvars
export HF_HOME="/home/project/11003280/vault/cache_yuli"
source /home/users/nus/huangyl/scratch/code/nscc_working/arf/mosaicml_workspace/credential/hf_token
source activate /home/project/11003280/envs/llmfoundry_yuli

# Change directory to the directory containing the data preparation script
cd /home/project/11003280/yuli/llm-foundry/scripts/data_prep
sleep 30


##############################
# gemma2b-8192
tokenizer="/home/users/nus/huangyl/scratch/model/gemma-2b-tokenizer"

# en_pile, doing
# th_pile1, doing
# th_pile2, doing
# th_hq, doing
# vi_pile, doing
# vi_hq, doing

python convert_dataset_json_mp.py \
    --path "/home/project/11003280/data/raw_hero/en_pile" \
    --out_root "/home/project/11003280/data/out_hero_gemma2/en_pile/" \
    --num_processes 100 \
    --concat_tokens 8192 \
    --tokenizer $tokenizer \
    --eos_text "<eos>" \
    --compression "zstd" \
    --chunk_size 300000 \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/en_hq/enwiki_dedup.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_gemma2/en_hq/" \
#     --num_processes 20 \
#     --concat_tokens 8192 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<eos>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/th_pile1/train.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_gemma2/th_pile1/" \
#     --num_processes 55 \
#     --concat_tokens 8192 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<eos>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/th_pile2/charin.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_gemma2/th_pile2/" \
#     --num_processes 35 \
#     --concat_tokens 8192 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<eos>" \
#     --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/th_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_gemma2/th_hq/" \
#     --num_processes 2 \
#     --concat_tokens 8192 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<eos>" \
#     --compression "zstd" \

python convert_dataset_json_mp.py \
    --path "/home/project/11003280/data/raw_hero/vi_pile/merged.jsonl" \
    --out_root "/home/project/11003280/data/out_hero_gemma2/vi_pile/" \
    --num_processes 100 \
    --concat_tokens 8192 \
    --tokenizer $tokenizer \
    --chunk_size 300000 \
    --eos_text "<eos>" \
    --compression "zstd" \

# python convert_dataset_json_mp.py \
#     --path "/home/project/11003280/data/raw_hero/vi_hq/combined.jsonl" \
#     --out_root "/home/project/11003280/data/out_hero_gemma2/vi_hq/" \
#     --num_processes 2 \
#     --concat_tokens 8192 \
#     --tokenizer $tokenizer \
#     --chunk_size 300000 \
#     --eos_text "<eos>" \
#     --compression "zstd" \


